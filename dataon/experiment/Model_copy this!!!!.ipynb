{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43e363d-2e75-4d6b-997b-1563d25bcd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2318842f-cc4e-4c8f-9feb-5c1b8ab228eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockingTimeSeriesSplit():\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "    \n",
    "    def get_n_splits(self, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        k_fold_size = n_samples // self.n_splits\n",
    "        indices = np.arange(n_samples)\n",
    "\n",
    "        margin = 0\n",
    "        for i in range(self.n_splits):\n",
    "            start = i * k_fold_size\n",
    "            stop = start + k_fold_size\n",
    "            mid = int(0.8 * (stop - start)) + start\n",
    "            yield indices[start: mid], indices[mid + margin: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642ec7b5-a972-4719-b0ae-2cd56c61e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class minmax():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def minmax_transform_fit(self, train_df, valid_df):\n",
    "\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler = scaler.fit(train_df)\n",
    "\n",
    "        train_scaled = scaler.transform(train_df)\n",
    "        train_scaled = pd.DataFrame(train_scaled)\n",
    "\n",
    "        valid_scaled = scaler.transform(valid_df)\n",
    "        valid_scaled = pd.DataFrame(valid_scaled)\n",
    "\n",
    "        return scaler, train_scaled, valid_scaled\n",
    "    \n",
    "    def minmax_transform(self, scaler, test_df):\n",
    "        \n",
    "        test_scaled = scaler.transform(test_df)\n",
    "        test_scaled = pd.DataFrame(test_scaled)\n",
    "\n",
    "        return test_scaled\n",
    "    \n",
    "    def inverse_minmax(self, scaler, df):\n",
    "        return scaler.inverse_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a56a970-7216-44a2-b931-6d28529189e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myPCA():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def PCA_transform_fit(self, train_df, valid_df):\n",
    "        n = len(train_df.columns)\n",
    "        self.pca = PCA(n_components = n)\n",
    "        pca_train = self.pca.fit_transform(train_df)\n",
    "        self.pca_train_df = pd.DataFrame(pca_train, columns = [f\"pca{num + 1}\" for num in range(n)])\n",
    "\n",
    "        eig = self.pca.explained_variance_\n",
    "        ratio = self.pca.explained_variance_ratio_\n",
    "        acc_ratio = ratio.cumsum()\n",
    "\n",
    "        self.pca_info = pd.DataFrame({\n",
    "            \"eig for variance\": eig,\n",
    "            \"acc. explained\": acc_ratio},\n",
    "            index = np.array([f\"pca{num + 1}\" for num in range(n)])\n",
    "        )\n",
    "\n",
    "        pca_valid = self.pca.transform(valid_df)\n",
    "        self.pca_valid_df = pd.DataFrame(pca_valid, columns = [f\"pca{num + 1}\" for num in range(n)])\n",
    "\n",
    "        return self.pca_train_df, self.pca_valid_df\n",
    "    \n",
    "    def PCA_transform(self, test_df):\n",
    "        \n",
    "        n = len(test_df.columns)\n",
    "        pca_test = self.pca.transform(test_df)\n",
    "        self.pca_test_df = pd.DataFrame(pca_test, columns = [f\"pca{num + 1}\" for num in range(n)])\n",
    "        \n",
    "        return self.pca_test_df\n",
    "\n",
    "# eigenvalue for variance >= 0.70\n",
    "# accumulated ratio of variance explained >= 0.80\n",
    "\n",
    "    def set_params(self):\n",
    "        for i in range(len(self.pca_train_df.columns)):\n",
    "            if (self.pca_info[\"acc. explained\"][i] >= 0.80):\n",
    "                self.param = i+1\n",
    "                return self.param\n",
    "        return len(self.pca_train_df.columns)-1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "165562ae-be04-4e03-88e2-3fa3daa55b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_minmax:\n",
    "    \n",
    "    matching_dict = {\"1018640\" : [\"402\", \"403\", \"413\"], \"1018662\": [\"403\", \"413\", \"421\"], \"1018680\": [\"415\", \"510\", \"889\"], \"1018683\": [\"415\", \"510\", \"889\"]}\n",
    "    \n",
    "    def __init__(self, code): #my_type: 0 (new fitting ... cv_num = 7, need to split test and train set) 1 (final fitting ... cv_num = 1, don't need to split test and train set)\n",
    "        # code: water level region code\n",
    "        # comp: component made by mstl\n",
    "        self.code = code\n",
    "        #self.my_type = my_type\n",
    "        # if my_type == 0:\n",
    "        #     self.num = 8 # blocking_cv num ... considering test set, set cv_num 8.\n",
    "        # elif my_type == 1:\n",
    "        #     self.num = 1\n",
    "        \n",
    "    def merging(self):\n",
    "        \n",
    "        self.loc_code = self.matching_dict[self.code]\n",
    "        \n",
    "        resid_lvl = pd.read_csv(f\"./data_MSTL/MSTLtest_lvl_{self.code}_resid.csv\")\n",
    "        #se24_lvl = pd.read_csv(f\"./data_MSTL/MSTL3_lvl_{self.code}_se24.csv\")\n",
    "        #se360_lvl = pd.read_csv(f\"./data_MSTL/MSTL3_lvl_{self.code}_se360.csv\")\n",
    "        se8760_lvl = pd.read_csv(f\"./data_MSTL/MSTLtest_lvl_{self.code}_se8760.csv\")\n",
    "        trend_lvl = pd.read_csv(f\"./data_MSTL/MSTLtest_lvl_{self.code}_trend.csv\")\n",
    "        \n",
    "        resid_fir = pd.read_csv(f\"./data_MSTL/MSTLtest_climate_{self.loc_code[0]}_resid.csv\")\n",
    "        #se24_fir = pd.read_csv(f\"./data_MSTL/MSTL3_climate_{self.loc_code[0]}_se24.csv\")\n",
    "        #se360_fir = pd.read_csv(f\"./data_MSTL/MSTL3_climate_{self.loc_code[0]}_se360.csv\")\n",
    "        se8760_fir = pd.read_csv(f\"./data_MSTL/MSTLtest_climate_{self.loc_code[0]}_se8760.csv\")\n",
    "        trend_fir = pd.read_csv(f\"./data_MSTL/MSTLtest_climate_{self.loc_code[0]}_trend.csv\")\n",
    "        \n",
    "        resid_sec = pd.read_csv(f\"./data_MSTL/MSTLtest_climate_{self.loc_code[1]}_resid.csv\")\n",
    "        #se24_sec = pd.read_csv(f\"./data_MSTL/MSTL3_climate_{self.loc_code[1]}_se24.csv\")\n",
    "        #se360_sec = pd.read_csv(f\"./data_MSTL/MSTL_climate_{self.loc_code[1]}_se360.csv\")\n",
    "        se8760_sec = pd.read_csv(f\"./data_MSTL/MSTLtest_climate_{self.loc_code[1]}_se8760.csv\")\n",
    "        trend_sec = pd.read_csv(f\"./data_MSTL/MSTLtest_climate_{self.loc_code[1]}_trend.csv\")\n",
    "        \n",
    "        resid_thr = pd.read_csv(f\"./data_MSTL/MSTLtest_climate_{self.loc_code[2]}_resid.csv\")\n",
    "        #se24_thr = pd.read_csv(f\"./data_MSTL/MSTL3_climate_{self.loc_code[2]}_se24.csv\")\n",
    "        #se360_thr = pd.read_csv(f\"./data_MSTL/MSTL3_climate_{self.loc_code[2]}_se360.csv\")\n",
    "        se8760_thr = pd.read_csv(f\"./data_MSTL/MSTLtest_climate_{self.loc_code[2]}_se8760.csv\")\n",
    "        trend_thr = pd.read_csv(f\"./data_MSTL/MSTLtest_climate_{self.loc_code[2]}_trend.csv\")\n",
    "        \n",
    "        resid_lvl.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        #se24_lvl.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        #se360_lvl.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        se8760_lvl.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        trend_lvl.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        \n",
    "        resid_fir.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        # se24_fir.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        #se360_fir.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        se8760_fir.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        trend_fir.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        \n",
    "        resid_sec.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        # se24_sec.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        #se360_sec.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        se8760_sec.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        trend_sec.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        \n",
    "        resid_thr.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        # se24_thr.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        #se360_thr.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        se8760_thr.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        trend_thr.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "        \n",
    "        resid_lvl.columns = [\"lvl_re\"]\n",
    "        # se24_lvl.columns = [\"lvl_se24\"]\n",
    "        #se360_lvl.columns = [\"lvl_se360\"]\n",
    "        se8760_lvl.columns = [\"lvl_se8760\"]\n",
    "        trend_lvl.columns = [\"lvl_tr\"]\n",
    "        \n",
    "        #print(resid_fir)\n",
    "        print(self.loc_code[0])\n",
    "        \n",
    "        resid_fir.columns = [\"fir_re_temp\", \"fir_re_pre\", \"fir_re_ver\", \"fir_re_hor\"]\n",
    "        # se24_fir.columns = [\"fir_se24_temp\", \"fir_se24_pre\", \"fir_se24_ver\", \"fir_se24_hor\"]\n",
    "        #se360_fir.columns = [\"fir_se360_temp\", \"fir_se360_pre\", \"fir_se360_ver\", \"fir_se360_hor\"]\n",
    "        se8760_fir.columns = [\"fir_se8760_temp\", \"fir_se8760_pre\", \"fir_se8760_ver\", \"fir_se8760_hor\"]\n",
    "        trend_fir.columns = [\"fir_tr_temp\", \"fir_tr_pre\", \"fir_tr_ver\", \"fir_tr_hor\"]\n",
    "        \n",
    "        resid_sec.columns = [\"sec_re_temp\", \"sec_re_pre\", \"sec_re_ver\", \"sec_re_hor\"]\n",
    "        # se24_sec.columns = [\"sec_se24_temp\", \"sec_se24_pre\", \"sec_se24_ver\", \"sec_se24_hor\"]\n",
    "        #se360_sec.columns = [\"sec_se360_temp\", \"sec_se360_pre\", \"sec_se360_ver\", \"sec_se360_hor\"]\n",
    "        se8760_sec.columns = [\"sec_se8760_temp\", \"sec_se8760_pre\", \"sec_se8760_ver\", \"sec_se8760_hor\"]\n",
    "        trend_sec.columns = [\"sec_tr_temp\", \"sec_tr_pre\", \"sec_tr_ver\", \"sec_tr_hor\"]\n",
    "        \n",
    "        resid_thr.columns = [\"thr_re_temp\", \"thr_re_pre\", \"thr_re_ver\", \"thr_re_hor\"]\n",
    "        # se24_thr.columns = [\"thr_se24_temp\", \"thr_se24_pre\", \"thr_se24_ver\", \"thr_se24_hor\"]\n",
    "        #se360_thr.columns = [\"thr_se360_temp\", \"thr_se360_pre\", \"thr_se360_ver\", \"thr_se360_hor\"]\n",
    "        se8760_thr.columns = [\"thr_se8760_temp\", \"thr_se8760_pre\", \"thr_se8760_ver\", \"thr_se8760_hor\"]\n",
    "        trend_thr.columns = [\"thr_tr_temp\", \"thr_tr_pre\", \"thr_tr_ver\", \"thr_tr_hor\"]\n",
    "        \n",
    "        \n",
    "        # self.merged_df = pd.concat([resid_lvl, se24_lvl, se360_lvl, se8760_lvl, trend_lvl,\n",
    "        #                            resid_fir, se24_fir, se360_fir, se8760_fir, trend_fir,\n",
    "        #                            resid_sec, se24_sec, se360_sec, se8760_sec, trend_sec,\n",
    "        #                            resid_thr, se24_thr, se360_thr, se8760_thr, trend_thr], axis = 1)\n",
    "            \n",
    "        self.merged_df = pd.concat([resid_lvl, se8760_lvl, trend_lvl,\n",
    "                                   resid_fir, se8760_fir, trend_fir,\n",
    "                                   resid_sec, se8760_sec, trend_sec,\n",
    "                                   resid_thr, se8760_thr, trend_thr], axis = 1)\n",
    "        \n",
    "        self.merged_df = self.merged_df.dropna()\n",
    "\n",
    "        #merged_df.head(30)\n",
    "        \n",
    "    def make_ind_set(self, num):\n",
    "        blocking_instance = BlockingTimeSeriesSplit(n_splits = num)\n",
    "        split_data = blocking_instance.split(self.merged_df)\n",
    "        ind_set = []\n",
    "        for i in range(num):\n",
    "            ind_set += next(split_data)\n",
    "        return ind_set\n",
    "        \n",
    "    def do_blocking(self):\n",
    "        (self.full_data, self.full_data_test) = self.blocking(8)\n",
    "        (self.full_data_final_test, _) = self.blocking(1)\n",
    "        \n",
    "    def do_minmax_and_pca(self):\n",
    "        self.minmax_and_pca(8)\n",
    "        \n",
    "    def blocking(self, num):\n",
    "        \n",
    "        ind_set = self.make_ind_set(num)\n",
    "        \n",
    "        # trend = []; seasonal_24 = []; seasonal_360 = []; seasonal_8760 = []; residual = []\n",
    "        trend = []; seasonal_8760 = []; residual = []\n",
    "        trend_test = []; seasonal_8760_test = []; residual_test = []\n",
    "\n",
    "        for i, ind in enumerate(ind_set):\n",
    "            if i%2 == 0:\n",
    "                train_df = self.merged_df.loc[ind]\n",
    "                train_df.reset_index()\n",
    "                # x_train = train_df.drop(['lvl_tr', 'lvl_se24', 'lvl_se360', 'lvl_se8760', 'lvl_re'], axis=1)\n",
    "                x_train = train_df.drop(['lvl_tr', 'lvl_se8760', 'lvl_re'], axis=1)\n",
    "            \n",
    "                x_train_tr = train_df[[\n",
    "                    \"fir_tr_temp\", \"fir_tr_pre\", \"fir_tr_ver\", \"fir_tr_hor\",\n",
    "                    \"sec_tr_temp\", \"sec_tr_pre\", \"sec_tr_ver\", \"sec_tr_hor\",\n",
    "                    \"thr_tr_temp\", \"thr_tr_pre\", \"thr_tr_ver\", \"thr_tr_hor\"\n",
    "                ]]\n",
    "                # x_train_se24 = train_df[[\n",
    "                #     \"fir_se24_temp\", \"fir_se24_pre\", \"fir_se24_ver\", \"fir_se24_hor\",\n",
    "                #     \"sec_se24_temp\", \"sec_se24_pre\", \"sec_se24_ver\", \"sec_se24_hor\",\n",
    "                #     \"thr_se24_temp\", \"thr_se24_pre\", \"thr_se24_ver\", \"thr_se24_hor\"\n",
    "                # ]]\n",
    "                # x_train_se360 = train_df[[\n",
    "                #     \"fir_se360_temp\", \"fir_se360_pre\", \"fir_se360_ver\", \"fir_se360_hor\",\n",
    "                #     \"sec_se360_temp\", \"sec_se360_pre\", \"sec_se360_ver\", \"sec_se360_hor\",\n",
    "                #     \"thr_se360_temp\", \"thr_se360_pre\", \"thr_se360_ver\", \"thr_se360_hor\"\n",
    "                # ]]\n",
    "                x_train_se8760 = train_df[[\n",
    "                    \"fir_se8760_temp\", \"fir_se8760_pre\", \"fir_se8760_ver\", \"fir_se8760_hor\",\n",
    "                    \"sec_se8760_temp\", \"sec_se8760_pre\", \"sec_se8760_ver\", \"sec_se8760_hor\",\n",
    "                    \"thr_se8760_temp\", \"thr_se8760_pre\", \"thr_se8760_ver\", \"thr_se8760_hor\"\n",
    "                ]]\n",
    "                x_train_re = train_df[[\n",
    "                    \"fir_re_temp\", \"fir_re_pre\", \"fir_re_ver\", \"fir_re_hor\",\n",
    "                    \"sec_re_temp\", \"sec_re_pre\", \"sec_re_ver\", \"sec_re_hor\",\n",
    "                    \"thr_re_temp\", \"thr_re_pre\", \"thr_re_ver\", \"thr_re_hor\"\n",
    "                ]]\n",
    "                y_train_tr = train_df[['lvl_tr']]\n",
    "                # y_train_se24 = train_df[['lvl_se24']]\n",
    "                # y_train_se360 = train_df[['lvl_se360']]\n",
    "                y_train_se8760 = train_df[['lvl_se8760']]\n",
    "                y_train_re = train_df[['lvl_re']]\n",
    "            else:\n",
    "                valid_df = self.merged_df.loc[ind]\n",
    "                valid_df.reset_index()\n",
    "                \n",
    "                #x_valid = valid_df.drop(['lvl_tr', 'lvl_se24', 'lvl_se360', 'lvl_se8760', 'lvl_re'], axis=1)\n",
    "                x_valid = valid_df.drop(['lvl_tr', 'lvl_se8760', 'lvl_re'], axis=1)\n",
    "                \n",
    "                x_valid_tr = valid_df[[\n",
    "                    \"fir_tr_temp\", \"fir_tr_pre\", \"fir_tr_ver\", \"fir_tr_hor\",\n",
    "                    \"sec_tr_temp\", \"sec_tr_pre\", \"sec_tr_ver\", \"sec_tr_hor\",\n",
    "                    \"thr_tr_temp\", \"thr_tr_pre\", \"thr_tr_ver\", \"thr_tr_hor\"\n",
    "                ]]\n",
    "                # x_valid_se24 = valid_df[[\n",
    "                #     \"fir_se24_temp\", \"fir_se24_pre\", \"fir_se24_ver\", \"fir_se24_hor\",\n",
    "                #     \"sec_se24_temp\", \"sec_se24_pre\", \"sec_se24_ver\", \"sec_se24_hor\",\n",
    "                #     \"thr_se24_temp\", \"thr_se24_pre\", \"thr_se24_ver\", \"thr_se24_hor\"\n",
    "                # ]]\n",
    "                # x_valid_se360 = valid_df[[\n",
    "                #     \"fir_se360_temp\", \"fir_se360_pre\", \"fir_se360_ver\", \"fir_se360_hor\",\n",
    "                #     \"sec_se360_temp\", \"sec_se360_pre\", \"sec_se360_ver\", \"sec_se360_hor\",\n",
    "                #     \"thr_se360_temp\", \"thr_se360_pre\", \"thr_se360_ver\", \"thr_se360_hor\"\n",
    "                # ]]\n",
    "                x_valid_se8760 = valid_df[[\n",
    "                    \"fir_se8760_temp\", \"fir_se8760_pre\", \"fir_se8760_ver\", \"fir_se8760_hor\",\n",
    "                    \"sec_se8760_temp\", \"sec_se8760_pre\", \"sec_se8760_ver\", \"sec_se8760_hor\",\n",
    "                    \"thr_se8760_temp\", \"thr_se8760_pre\", \"thr_se8760_ver\", \"thr_se8760_hor\"\n",
    "                ]]\n",
    "                x_valid_re = valid_df[[\n",
    "                    \"fir_re_temp\", \"fir_re_pre\", \"fir_re_ver\", \"fir_re_hor\",\n",
    "                    \"sec_re_temp\", \"sec_re_pre\", \"sec_re_ver\", \"sec_re_hor\",\n",
    "                    \"thr_re_temp\", \"thr_re_pre\", \"thr_re_ver\", \"thr_re_hor\"\n",
    "                ]]\n",
    "                y_valid_tr = valid_df[['lvl_tr']]\n",
    "                # y_valid_se24 = valid_df[['lvl_se24']]\n",
    "                # y_valid_se360 = valid_df[['lvl_se360']]\n",
    "                y_valid_se8760 = valid_df[['lvl_se8760']]\n",
    "                y_valid_re = valid_df[['lvl_re']]\n",
    "                \n",
    "                if (i == 2 * num - 1 and i > 1):\n",
    "                    x_test_tr = pd.concat([x_train_tr, x_valid_tr], ignore_index=True)\n",
    "                    y_test_tr = pd.concat([y_train_tr, y_valid_tr], ignore_index=True)\n",
    "                    x_test_se8760 = pd.concat([x_train_se8760, x_valid_se8760], ignore_index=True)\n",
    "                    y_test_se8760 = pd.concat([y_train_se8760, y_valid_se8760], ignore_index=True)\n",
    "                    x_test_re = pd.concat([x_train_re, x_valid_re], ignore_index=True)\n",
    "                    y_test_re = pd.concat([y_train_re, y_valid_re], ignore_index=True)\n",
    "                    trend_test.append([x_test_tr, y_test_tr])\n",
    "                    seasonal_8760_test.append([x_test_se8760, y_test_se8760])\n",
    "                    residual_test.append([x_test_re, y_test_re])\n",
    "                    \n",
    "                else:\n",
    "                    trend.append([x_train_tr, y_train_tr, x_valid_tr, y_valid_tr])\n",
    "                    # seasonal_24.append([x_train_se24, y_train_se24, x_valid_se24, y_valid_se24])\n",
    "                    # seasonal_360.append([x_train_se360, y_train_se360, x_valid_se360, y_valid_se360])\n",
    "                    seasonal_8760.append([x_train_se8760, y_train_se8760, x_valid_se8760, y_valid_se8760])\n",
    "                    residual.append([x_train_re, y_train_re, x_valid_re, y_valid_re]) \n",
    "        \n",
    "        # self.full_data = [trend, seasonal_24, seasonal_360, seasonal_8760, residual]\n",
    "        full_data = [trend, seasonal_8760, residual]\n",
    "        full_data_test = [trend_test, seasonal_8760_test, residual_test]\n",
    "        \n",
    "        return (full_data, full_data_test)\n",
    "        \n",
    "        \n",
    "        #self.full_data_final_test = [trend_test, seasonal_8760_test, residual_test]\n",
    "    \n",
    "    def minmax_and_pca(self, num):\n",
    "        \n",
    "        self.pca_full_data = self.full_data\n",
    "        self.pca_full_data_test = self.full_data_test\n",
    "        self.pca_full_data_final_test = self.full_data_final_test\n",
    "        \n",
    "        self.minmax = minmax()\n",
    "        \n",
    "        self.scaler_X = []\n",
    "        self.scaler_y = []\n",
    "        \n",
    "        ind_set = self.make_ind_set(num)[:-2]\n",
    "        \n",
    "        for i in range(len(self.full_data)):\n",
    "            \n",
    "            input_df0 = pd.DataFrame([])\n",
    "            input_df1 = pd.DataFrame([])\n",
    "            input_df2 = pd.DataFrame([])\n",
    "            input_df3 = pd.DataFrame([])\n",
    "            \n",
    "            input_df0_test = pd.DataFrame([])\n",
    "            input_df1_test = pd.DataFrame([])\n",
    "            \n",
    "            input_df0_final_test = pd.DataFrame([])\n",
    "            input_df1_final_test = pd.DataFrame([])\n",
    "            input_df2_final_test = pd.DataFrame([])\n",
    "            input_df3_final_test = pd.DataFrame([])\n",
    "            \n",
    "            for j in range(len(self.full_data[0])):\n",
    "                input_df0 = pd.concat([input_df0, self.full_data[i][j][0]]) # x train\n",
    "                input_df1 = pd.concat([input_df1, self.full_data[i][j][1]]) # y train\n",
    "                input_df2 = pd.concat([input_df2, self.full_data[i][j][2]]) # x valid\n",
    "                input_df3 = pd.concat([input_df3, self.full_data[i][j][3]]) # y valid\n",
    "                \n",
    "            input_df0_test = self.full_data_test[i][0][0] # x test\n",
    "            input_df1_test =self.full_data_test[i][0][1] # y test\n",
    "                \n",
    "            input_df0_final_test = self.full_data_final_test[i][0][0] \n",
    "            input_df1_final_test = self.full_data_final_test[i][0][1] \n",
    "            input_df2_final_test = self.full_data_final_test[i][0][2] \n",
    "            input_df3_final_test = self.full_data_final_test[i][0][3] \n",
    "            \n",
    "            (scaler, df0, df2) = self.minmax.minmax_transform_fit(input_df0, input_df2) # x\n",
    "            self.scaler_X.append(scaler)\n",
    "            \n",
    "            df0_test = self.minmax.minmax_transform(scaler, input_df0_test)\n",
    "            df0_final_test = self.minmax.minmax_transform(scaler, input_df0_final_test)\n",
    "            df2_final_test = self.minmax.minmax_transform(scaler, input_df2_final_test)\n",
    "            \n",
    "            (scaler, df1, df3) = self.minmax.minmax_transform_fit(input_df1, input_df3) # y\n",
    "            self.scaler_y.append(scaler)\n",
    "            \n",
    "            df1_test = self.minmax.minmax_transform(scaler, input_df1_test)\n",
    "            df1_final_test = self.minmax.minmax_transform(scaler, input_df1_final_test)\n",
    "            df3_final_test = self.minmax.minmax_transform(scaler, input_df3_final_test)\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.pca_instance = myPCA()\n",
    "            x_train_pca, x_valid_pca = self.pca_instance.PCA_transform_fit(df0, df2)\n",
    "            no = self.pca_instance.set_params()\n",
    "            x_train_pca, x_valid_pca = x_train_pca[[f\"pca{d+1}\" for d in range(no)]], x_valid_pca[[f\"pca{d+1}\" for d in range(no)]]\n",
    "            y_train_pca, y_valid_pca = (df1, df3)\n",
    "            \n",
    "            x_test_pca = self.pca_instance.PCA_transform(df0_test)\n",
    "            x_test_pca = x_test_pca[[f\"pca{d+1}\" for d in range(no)]]\n",
    "            x_final_train_pca = self.pca_instance.PCA_transform(df0_final_test)\n",
    "            x_final_train_pca = x_final_train_pca[[f\"pca{d+1}\" for d in range(no)]]\n",
    "            x_final_valid_pca = self.pca_instance.PCA_transform(df2_final_test)\n",
    "            x_final_valid_pca = x_final_valid_pca[[f\"pca{d+1}\" for d in range(no)]]\n",
    "            \n",
    "            y_test_pca = df1_test\n",
    "            y_final_train_pca = df1_final_test\n",
    "            y_final_valid_pca = df3_final_test\n",
    "\n",
    "            delta1 = len(ind_set[0])\n",
    "            delta2 = len(ind_set[1])\n",
    "            \n",
    "            for k, ind in enumerate(ind_set):\n",
    "                if k % 2 == 0:\n",
    "                    self.pca_full_data[i][k//2][0] = x_train_pca.loc[[ind[i] - delta2 * k//2 for i in range(len(ind))]] # x train\n",
    "                    self.pca_full_data[i][k//2][0].reset_index(inplace=True, drop = True)\n",
    "                    self.pca_full_data[i][k//2][1] = y_train_pca.loc[[ind[i] - delta2 * k//2 for i in range(len(ind))]] # y train\n",
    "                    self.pca_full_data[i][k//2][1].reset_index(inplace=True, drop = True)\n",
    "                else:\n",
    "                    self.pca_full_data[i][k//2][2] = x_valid_pca.loc[[ind[i] - delta1 * (k//2 + 1) for i in range(len(ind))]] # x valid\n",
    "                    self.pca_full_data[i][k//2][2].reset_index(inplace=True, drop = True)\n",
    "                    self.pca_full_data[i][k//2][3] = y_valid_pca.loc[[ind[i] - delta1 * (k//2 + 1) for i in range(len(ind))]] # y valid \n",
    "                    self.pca_full_data[i][k//2][3].reset_index(inplace=True, drop = True)\n",
    "        \n",
    "            self.pca_full_data_test[i][0][0] = x_test_pca\n",
    "            self.pca_full_data_test[i][0][0].reset_index(inplace=True, drop = True)\n",
    "            self.pca_full_data_test[i][0][1] = y_test_pca\n",
    "            self.pca_full_data_test[i][0][1].reset_index(inplace=True, drop = True)\n",
    "            \n",
    "            self.pca_full_data_final_test[i][0][0] = x_final_train_pca\n",
    "            self.pca_full_data_final_test[i][0][0].reset_index(inplace=True, drop = True)\n",
    "            self.pca_full_data_final_test[i][0][1] = y_final_train_pca\n",
    "            self.pca_full_data_final_test[i][0][1].reset_index(inplace=True, drop = True)\n",
    "            self.pca_full_data_final_test[i][0][2] = x_final_valid_pca\n",
    "            self.pca_full_data_final_test[i][0][2].reset_index(inplace=True, drop = True)\n",
    "            self.pca_full_data_final_test[i][0][3] = y_final_valid_pca\n",
    "            self.pca_full_data_final_test[i][0][3].reset_index(inplace=True, drop = True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # (scaler, df0, df2) = self.normalization.normalize_transform_fit(input_df0, input_df2) # x\n",
    "            # self.scaler_X.append(scaler)\n",
    "            # (scaler, df1, df3) = self.normalization.normalize_transform_fit(input_df1, input_df3) # y\n",
    "            # self.scaler_y.append(scaler)\n",
    "            \n",
    "            #x_test_pca = self.pca_instance.PCA_transform(df2)\n",
    "            #x_train_pca, x_valid_pca = x_train_pca[[f\"pca{d+1}\" for d in range(no)]], x_valid_pca[[f\"pca{d+1}\" for d in range(no)]] \n",
    "                \n",
    "                \n",
    "                #self.pca_full_data[i].append([x_train_pca, y_train_pca, x_valid_pca, y_valid_pca])\n",
    "        \n",
    "        \n",
    "                 \n",
    "            # # for j in range(len(self.full_data[0])):\n",
    "            # #     (scaler, self.scaled_data[i][j][0], self.scaled_data[i][j][2]) = self.normalization.normalize(self.full_data[i][j][0], self.full_data[i][j][2])\n",
    "            # #     temp_X.append(scaler)\n",
    "            # #     (scaler, self.scaled_data[i][j][1], self.scaled_data[i][j][3]) = self.normalization.normalize(self.full_data[i][j][1], self.full_data[i][j][3])\n",
    "            # #     temp_y.append(scaler)\n",
    "            # self.scaler_X.append(temp_X)\n",
    "            # self.scaler_y.append(temp_y)\n",
    "            \n",
    "            \n",
    "    def inverse_minmax(self, pred): # df may be a prediction vector (3d array)\n",
    "        \n",
    "        return_pred = pred\n",
    "        \n",
    "        for i in range(3):\n",
    "            return_pred[i] = self.minmax.inverse_minmax(self.scaler_y[i], pred[i].reshape(1, -1))\n",
    "        #print(pred[0][0], return_pred[0][0])\n",
    "        return return_pred \n",
    "        \n",
    "#     def pca(self):\n",
    "        \n",
    "        \n",
    "#         self.scaled_data = self.full_data\n",
    "        \n",
    "#         self.normalization = normalization()\n",
    "        \n",
    "#         self.scaler_X = []\n",
    "#         self.scaler_y = []\n",
    "        \n",
    "#         ind_set = self.make_ind_set(num)[:-2]\n",
    "#         print(ind_set)\n",
    "        \n",
    "#         for i in range(len(self.full_data)):\n",
    "            \n",
    "#             input_df0 = pd.DataFrame([])\n",
    "#             input_df1 = pd.DataFrame([])\n",
    "#             input_df2 = pd.DataFrame([])\n",
    "#             input_df3 = pd.DataFrame([])\n",
    "#             for j in range(len(self.full_data[0])):\n",
    "#                 input_df0 = pd.concat([input_df0, self.full_data[i][j][0]]) # x train\n",
    "#                 input_df1 = pd.concat([input_df1, self.full_data[i][j][1]]) # y train\n",
    "#                 input_df2 = pd.concat([input_df2, self.full_data[i][j][2]]) # x valid\n",
    "#                 input_df3 = pd.concat([input_df3, self.full_data[i][j][3]]) # y valid\n",
    "            \n",
    "#             (scaler, df0, df2) = self.normalization.normalize(input_df0, input_df2) # x\n",
    "#             self.scaler_X.append(scaler)\n",
    "#             (scaler, df1, df3) = self.normalization.normalize(input_df1, input_df3) # y\n",
    "#             self.scaler_y.append(scaler)\n",
    "            \n",
    "#         pca_tr = []; pca_sn8760 = []; pca_re = []\n",
    "#         self.pca_full_data = [pca_tr, pca_sn8760, pca_re]\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # pca_tr = []; pca_sn24 = []; pca_sn360 = []; pca_sn8760 = []; pca_re = []\n",
    "#         pca_tr = []; pca_sn8760 = []; pca_re = []\n",
    "#         #self.pca_full_data = [pca_tr, pca_sn24, pca_sn360, pca_sn8760, pca_re]\n",
    "#         self.pca_full_data = [pca_tr, pca_sn8760, pca_re]\n",
    "\n",
    "#         for i, comp in enumerate(self.scaled_data):\n",
    "#             for split in range(self.num):\n",
    "#                 self.pca_instance = myPCA()\n",
    "#                 x_train_pca, x_valid_pca = self.pca_instance.result_PCA(comp[split][0], comp[split][2])\n",
    "#                 no = self.pca_instance.set_params()\n",
    "#                 x_train_pca, x_valid_pca = x_train_pca[[f\"pca{d+1}\" for d in range(no)]], x_valid_pca[[f\"pca{d+1}\" for d in range(no)]]\n",
    "#                 y_train_pca, y_valid_pca = (comp[split][1], comp[split][3])\n",
    "#                 self.pca_full_data[i].append([x_train_pca, y_train_pca, x_valid_pca, y_valid_pca])\n",
    "            \n",
    "#         return self.pca_full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77dd4529-1d33-452c-b7cf-bac24fa8399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 07:41:25.692368: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tools.eval_measures import rmse, aic\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# from pyFTS.common import Transformations, Util as cUtil, Membership as mf\n",
    "# from pyFTS.benchmarks import benchmarks as bchmk, Util as bUtil, Measures\n",
    "# from pyFTS.partitioners import CMeans, Grid, FCM, Huarng, Entropy, Util as pUtil\n",
    "# from pyFTS.models.multivariate import common, variable, mvfts\n",
    "# from pyFTS.models import hofts\n",
    "# from pyFTS.models import chen\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "import math\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "969c438a-cfd9-4eef-b800-e086642ca4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class predict_model:\n",
    "    \n",
    "    def __init__(self, df, code):\n",
    "        self.type_dict = {'trend': 0, 'seasonal': 1, 'residual': 2}\n",
    "        \n",
    "        self.dataset = df\n",
    "        self.code = code # 수위 예측 지점 코드\n",
    "        \n",
    "        self.trend_model = 0\n",
    "        \n",
    "        self.scaler_X = []\n",
    "        self.scaler_y = []\n",
    "        self.seasonal_model = 0\n",
    "        \n",
    "        self.df = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def SMAPE(y_test, y_pred):\n",
    "        y_test = y_test.to_numpy(); y_pred = y_pred.to_numpy()\n",
    "        return np.mean((np.abs(y_test-y_pred))/(np.abs(y_test)+np.abs(y_pred)))*100\n",
    "\n",
    "    @staticmethod\n",
    "    def RMSE(y_test, y_pred):\n",
    "        return np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    @staticmethod\n",
    "    def SMAPE_np(y_test, y_pred):\n",
    "        return np.mean((np.abs(y_test-y_pred))/(np.abs(y_test)+np.abs(y_pred)))*100\n",
    "    \n",
    "    @staticmethod\n",
    "    def RMSE_np(y_test, y_pred):\n",
    "        return np.mean((y_test - y_pred)**2)\n",
    "        \n",
    "    def fit_VAR_model(self, my_type, is_final, lag = 15, step = 3):\n",
    "        given_data = self.dataset[self.type_dict[my_type]] # trend\n",
    "        \n",
    "        least_rmse = 1000\n",
    "\n",
    "        for idx, block_data in enumerate(given_data):\n",
    "            X_train = block_data[0]\n",
    "            y_train = block_data[1]\n",
    "            X_valid = block_data[2]\n",
    "            y_valid = block_data[3]\n",
    "            \n",
    "            SMAPE_sum = 0\n",
    "            RMSE_sum = 0\n",
    "            \n",
    "            df_train = pd.merge(y_train, X_train, left_index=True, right_index=True, how='inner')\n",
    "            df_valid = pd.merge(y_valid, X_valid, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "            print(f\"Modeling {my_type} component... SPLIT %d\"%idx, end = \"\\r\")\n",
    "            premodel = VAR(df_train)\n",
    "            model = premodel.fit(lag)\n",
    "\n",
    "            df_valid = pd.concat([df_train.loc[-lag: , :], df_valid])\n",
    "            \n",
    "\n",
    "            for i in range(0, len(df_valid)-lag-step):\n",
    "                lagged_values = df_valid.values[i: i+lag]\n",
    "                forecast = pd.DataFrame(model.forecast(y = lagged_values, steps = step), columns=df_train.columns)\n",
    "                SMAPE_sum += self.SMAPE(forecast.iloc[:step, [0]], df_valid.iloc[i+lag: i+step+lag, [0]])\n",
    "                RMSE_sum += self.RMSE(forecast.iloc[:step, [0]], df_valid.iloc[i+lag: i+step+lag, [0]]) \n",
    "            \n",
    "            print(f\"TREND SPLIT {idx}: SMAPE avg {SMAPE_sum / (len(df_valid) - lag)}, RMSE avg {RMSE_sum / (len(df_valid) - lag)}\")\n",
    "            \n",
    "            if least_rmse > RMSE_sum / (len(df_valid) - lag):\n",
    "                least_rmse = RMSE_sum / (len(df_valid) - lag)\n",
    "                best_model = model\n",
    "                \n",
    "        if is_final == 1:\n",
    "            dump(best_model, f'{my_type}_{self.code}_model_final.joblib')\n",
    "        else:\n",
    "            dump(best_model, f'{my_type}_{self.code}_model.joblib')\n",
    "        \n",
    "    def eval_VAR_model(self, df, my_type):\n",
    "        \n",
    "        lag = 15; step = 3\n",
    "        \n",
    "        model = load(f'{my_type}_{self.code}_model.joblib')\n",
    "        \n",
    "        given_data = df[self.type_dict[my_type]]\n",
    "        \n",
    "        X_test = given_data[0][0]\n",
    "        y_test = given_data[0][1]\n",
    "        \n",
    "        df_test = pd.merge(y_test, X_test, left_index=True, right_index=True, how='inner')\n",
    "        \n",
    "        pred = []\n",
    "        \n",
    "        for i in range(0, len(df_test)-lag-step):\n",
    "            lagged_values = df_test.values[i: i+lag]\n",
    "            forecast = pd.DataFrame(model.forecast(y = lagged_values, steps = step), columns=df_test.columns)\n",
    "            pred.append([[forecast[0][0]], [forecast[0][1]], [forecast[0][2]]])\n",
    "            \n",
    "        return np.array(pred)\n",
    "\n",
    "\n",
    "    def pred_VAR_model(self, my_type):\n",
    "        \n",
    "        df = self.dataset\n",
    "        \n",
    "        lag = 15; step = 3\n",
    "        \n",
    "        model = load(f'{my_type}_{self.code}_model_final.joblib')\n",
    "        \n",
    "        given_data = df[0]\n",
    "        \n",
    "        X_test = given_data[0][0]\n",
    "        y_test = given_data[0][1]\n",
    "        \n",
    "        df_test = pd.merge(y_test, X_test, left_index=True, right_index=True, how='inner')\n",
    "        \n",
    "        pred = []\n",
    "        \n",
    "        i = len(df_test)-lag\n",
    "        lagged_values = df_test.values[i: i+lag]\n",
    "        forecast = pd.DataFrame(model.forecast(y = lagged_values, steps = step), columns=df_test.columns)\n",
    "        pred.append([[forecast[0][0]], [forecast[0][1]], [forecast[0][2]]])\n",
    "            \n",
    "        return np.array(pred)\n",
    "        \n",
    "    ###################         \n",
    "\n",
    "    def make_dataset(self, X_df, y_df, time_step = 30, y_step = 3):\n",
    "        new_X_df = []\n",
    "        new_y_df = []\n",
    "        X_df = pd.concat([X_df, y_df], axis=1)\n",
    "        for i in range(len(X_df) - time_step - y_step + 1):\n",
    "            new_X_df.append(np.array(X_df.iloc[i:i + time_step])) # i ~ i + time_step - 1\n",
    "            new_y_df.append(np.array(y_df.iloc[i + time_step: i + time_step + y_step])) # i + time_step ~ i + time_step + y_step - 1\n",
    "        return np.array(new_X_df), np.array(new_y_df)\n",
    "    \n",
    "    \n",
    "    def calculate_metrics(self, true, pred):\n",
    "        true = pd.DataFrame(true)\n",
    "        pred = pd.DataFrame(pred)\n",
    "        mae = (true - pred).abs().mean()\n",
    "        mape = (true - pred).abs().div(true).mean() * 100\n",
    "        rmse = np.sqrt(((true - pred) ** 2).mean())\n",
    "        return {\n",
    "            \"mae\": mae,\n",
    "            \"mape\": mape,\n",
    "            \"rmse\": rmse,\n",
    "        }\n",
    "    \n",
    "    def Bi_Seq2_model(self, feature_num, time_step, y_step):\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Bidirectional(keras.layers.LSTM(128, activation = 'tanh', input_shape = (time_step, feature_num), return_sequences = True, kernel_initializer='he_normal')))\n",
    "        model.add(keras.layers.Bidirectional(keras.layers.LSTM(64, activation = 'tanh')))\n",
    "        model.add(keras.layers.RepeatVector(y_step))\n",
    "        model.add(keras.layers.LSTM(64, activation = 'tanh', return_sequences = True))\n",
    "        model.add(keras.layers.LSTM(128, activation = 'tanh', return_sequences = True))\n",
    "        model.add(keras.layers.TimeDistributed(keras.layers.Dense(1)))\n",
    "        #model.compile(optimizer = 'nadam', loss = 'mse')\n",
    "        #model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def Bi_Seq2_fit(self, model, X_train, y_train, X_valid, y_valid, time_step = 8, patience = 50, epochs = 1000):\n",
    "\n",
    "        model.compile(loss='mean_squared_error', optimizer = 'nadam')\n",
    "        early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience = patience)\n",
    "\n",
    "        history = model.fit(X_train, y_train, \n",
    "                            epochs = epochs,\n",
    "                            validation_data = (X_valid, y_valid), \n",
    "                            callbacks = [early_stop],\n",
    "                            shuffle = False)\n",
    "        return history\n",
    "    \n",
    "    def fit_Bi_Seq2(self, my_type, is_final, time_step = 30, y_step = 3, patience = 100, epochs = 1000):\n",
    "        \n",
    "        self.scaled_data = self.dataset[self.type_dict[my_type]]\n",
    "        \n",
    "        pred = []\n",
    "        \n",
    "        with tf.device('/device:XLA_GPU:0'):\n",
    "            least_rmse = 1000\n",
    "            for i in range(len(self.dataset[0])):\n",
    "                (lstm_X_train, lstm_y_train) = self.make_dataset(self.scaled_data[i][0], self.scaled_data[i][1], time_step)\n",
    "                (lstm_X_valid, lstm_y_valid) = self.make_dataset(self.scaled_data[i][2], self.scaled_data[i][3], time_step)\n",
    "\n",
    "                model = self.Bi_Seq2_model(len(lstm_X_train[0][0]), time_step, y_step)\n",
    "\n",
    "                history = self.Bi_Seq2_fit(model, lstm_X_train, lstm_y_train, lstm_X_valid, lstm_y_valid, time_step, patience, epochs)\n",
    "\n",
    "                pred_temp = model.predict(lstm_X_valid)\n",
    "\n",
    "                rmse = 0\n",
    "\n",
    "                for j in range(y_step):\n",
    "                    pred.append(pred_temp)\n",
    "                    mat = self.calculate_metrics(lstm_y_valid[:, j], pred_temp[:, j])\n",
    "                    print(mat)\n",
    "                    rmse = rmse + mat['rmse'][0]\n",
    "\n",
    "                    plt.plot(lstm_y_valid[:, j], label = 'actual')\n",
    "                    plt.plot(pred_temp[:, j], label = 'prediction')\n",
    "                        # print(self.calculate_metrics(self.inverse_min_max_scaling(self.scaler_y[i], lstm_y_valid[:, j]), self.inverse_min_max_scaling(self.scaler_y[i], pred_temp[:, j])))\n",
    "                        # plt.plot(self.inverse_min_max_scaling(self.scaler_y[i], lstm_y_valid[:, j]), label = 'actual')\n",
    "                        # plt.plot(self.inverse_min_max_scaling(self.scaler_y[i], pred_temp[:, j]), label = 'prediction')\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "\n",
    "                rmse = rmse / y_step\n",
    "\n",
    "                if rmse < least_rmse:\n",
    "                    least_rmse = rmse\n",
    "                    best_time_step = time_step\n",
    "                    best_model = model\n",
    "        \n",
    "        if is_final == 1:\n",
    "            best_model.save(f'{my_type}_{self.code}_model_final.h5')\n",
    "        else:\n",
    "            best_model.save(f'{my_type}_{self.code}_model.h5')\n",
    "    \n",
    "    def eval_Bi_Seq2(self, df, my_type): # 모델 평가를 위한 예측값을 도출하는 함수\n",
    "        time_step = 30\n",
    "        \n",
    "        model = tf.keras.models.load_model(f'{my_type}_{self.code}_model.h5')\n",
    "        \n",
    "        new_df = df[self.type_dict[my_type]]\n",
    "        (lstm_X_test, lstm_y_test) = self.make_dataset(new_df[0][0], new_df[0][1], time_step)\n",
    "        \n",
    "        pred = model.predict(lstm_X_test)\n",
    "        \n",
    "        return pred\n",
    "        \n",
    "    def pred_Bi_Seq2(self, my_type): # 실제 예측을 위한 함수\n",
    "        \n",
    "        df = self.dataset\n",
    "        \n",
    "        time_step = 30\n",
    "        \n",
    "        model = tf.keras.models.load_model(f'{my_type}_{self.code}_model_final.h5')\n",
    "        new_df = df[1]\n",
    "        lstm_X_test = pd.concat([new_df[0][0].iloc[-time_step:], new_df[0][1].iloc[-time_step:]], axis=1)\n",
    "        (lstm_X_temp, lstm_y_temp) = self.make_dataset(new_df[0][0], new_df[0][1], time_step)\n",
    "\n",
    "        lstm_X_test = np.array([lstm_X_test])\n",
    "\n",
    "        return model.predict(lstm_X_test)\n",
    "    \n",
    "    def eval_model(self, pre, trend_pred, seasonal_pred, resid_pred, df): #pre: instance of preprocessing class\n",
    "        \n",
    "        y_step = 3\n",
    "        lag = 15\n",
    "        time_step = 30\n",
    "        \n",
    "        trend_pred = trend_pred[time_step - lag - 1:]\n",
    "        \n",
    "        y_pred = np.concatenate([trend_pred, seasonal_pred, resid_pred], axis = 2)\n",
    "        \n",
    "        y_pred = y_pred.transpose((1,2,0))\n",
    "        \n",
    "        for i in range(len(y_pred)):\n",
    "            y_pred[i] = pre.inverse_minmax(y_pred[i])\n",
    "        #y_pred = np.transpose(np.sum(y_pred, axis=2))\n",
    "        y_pred = np.transpose(np.sum(y_pred, axis = 1))\n",
    "        \n",
    "        y_real_trend = np.array(df[0][0][1])\n",
    "        y_real_seasonal = np.array(df[1][0][1])\n",
    "        y_real_resid = np.array(df[2][0][1])\n",
    "        \n",
    "        y_real = np.concatenate([y_real_trend, y_real_seasonal, y_real_resid], axis = 1)\n",
    "        for i in range(len(y_real)):\n",
    "            y_real[i] = pre.inverse_minmax(y_real[i])\n",
    "        y_real = np.transpose(np.sum(y_real, axis = 1))\n",
    "        \n",
    "        y_real = y_real[time_step + y_step - 1:]\n",
    "        \n",
    "        eval_mat = []\n",
    "        \n",
    "        for i in range(y_step):\n",
    "            eval_mat.append(self.calculate_metrics(y_pred[:,i], y_real))\n",
    "            \n",
    "            plt.plot(y_real, label = 'actual')\n",
    "            plt.plot(y_pred[:,i], label = 'prediction')\n",
    "            plt.show()\n",
    "            \n",
    "        return eval_mat\n",
    "    \n",
    "    def save_prediction(self, trend_pred, seasonal_pred, resid_pred):\n",
    "        pd.DataFrame(trend_pred.reshape(3,1)).to_csv(path_or_buf=f'./prediction_data/{self.code}_trend_pred.csv')\n",
    "        pd.DataFrame(seasonal_pred.reshape(3,1)).to_csv(path_or_buf=f'./prediction_data/{self.code}_seasonal_pred.csv')\n",
    "        pd.DataFrame(residual_pred.reshape(3,1)).to_csv(path_or_buf=f'./prediction_data/{self.code}_residual_pred.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ac3fd-0a31-4a26-a08f-b9d41bad93f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n",
      "TREND SPLIT 0: SMAPE avg 0.005845939808003341, RMSE avg 8.93686890249827e-06\n"
     ]
    }
   ],
   "source": [
    "# 이거 돌리면 된다~~~\n",
    "#code_list = [\"1018680\", \"1018640\", \"1018662\", \"1018683\"]\n",
    "code_list = [\"1018662\"]\n",
    "for code in code_list:\n",
    "    pre = Preprocessing_minmax(code)\n",
    "    pre.merging()\n",
    "    pre.do_blocking()\n",
    "    pre.do_minmax_and_pca()\n",
    "    full_data = pre.pca_full_data\n",
    "    full_data_test = pre.pca_full_data_test\n",
    "    full_data_final_test = pre.pca_full_data_final_test\n",
    "    model = predict_model(full_data_final_test, code)\n",
    "    model.fit_VAR_model(\"trend\", is_final = 1)\n",
    "    model.fit_Bi_Seq2(\"seasonal\", is_final = 1)\n",
    "    model.fit_Bi_Seq2(\"residual\", is_final = 1)\n",
    "\n",
    "for code in code_list:\n",
    "    trend_pred = model.pred_VAR_model(\"trend\")\n",
    "    seasonal_lstm_pred = model.pred_Bi_Seq2(\"seasonal\")\n",
    "    residual_lstm_pred = model.pred_Bi_Seq2(\"residual\")\n",
    "    #seasonal_anfis_pred = model.pred_ANFIS_model(\"seasonal\")\n",
    "    #residual_anfis_pred = model.pred_ANFIS_model(\"residual\")\n",
    "    ## 적당히 seasonal, residual prediction 부분 합치기\n",
    "    #save_prediction(trend_pred, seasonal_pred, resid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cdf58f0-9994-48bf-9ba8-48009baa348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cbcefd7-b7d2-40ed-bd3a-3a03b5a8e2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e2c623c-a317-4a00-a844-fed20cc06d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6271030953856516466\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11740496968915732777\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9289073193153464267\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 426377216\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16643419242364102664\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-22 07:48:46.358263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2023-10-22 07:48:46.358495: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-10-22 07:48:46.358643: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2023-10-22 07:48:46.358704: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2023-10-22 07:48:46.358753: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2023-10-22 07:48:46.358800: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-10-22 07:48:46.358853: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-10-22 07:48:46.358920: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-10-22 07:48:46.361434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
      "2023-10-22 07:48:46.361491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-10-22 07:48:46.361504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
      "2023-10-22 07:48:46.361515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
      "2023-10-22 07:48:46.366806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/device:GPU:0 with 406 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58432baf-1daf-462e-be2a-a753f746bd11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
